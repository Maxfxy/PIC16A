{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Academic Integrity Statement\n",
    "\n",
    "As a matter of Departmental policy, **we are required to give you a 0** unless you **type your name** after the following statement: \n",
    "\n",
    "> *I certify on my honor that I have neither given nor received any help, or used any non-permitted resources, while completing this evaluation.*\n",
    "\n",
    "\\[TYPE YOUR NAME HERE\\]\n",
    "\n",
    "# Problem 1 (50 points)\n",
    "\n",
    "Rampant disinformation---often called \"fake news\"---has emerged as one of the fundamental crises over our time. \n",
    "\n",
    "<figure class=\"image\" style=\"width:30%\">\n",
    "  <img src=\"https://s3.amazonaws.com/libapps/accounts/63707/images/21392935.jpg\" alt=\"A portrait of Willy Wonka, wearing a purple suit and brown top hat. His face is condescending. The caption reads: 'Oh, so you read it on the internet? Well then I guess it must be true.'\">\n",
    "  <figcaption><i></i></figcaption>\n",
    "</figure>\n",
    "\n",
    "There is a growing movement for online platforms to regulate fake news. Doing so at scale requires combing through millions of news items every day, making it very expensive to do by hand. Can an algorithm do it instead? \n",
    "\n",
    "The following two URLs each contain part of a data set. \n",
    "\n",
    "- **Fake news items**: `https://raw.githubusercontent.com/PhilChodrow/PIC16A/master/datasets/fake_news/Fake.csv`\n",
    "- **Real news items**: `https://raw.githubusercontent.com/PhilChodrow/PIC16A/master/datasets/fake_news/true.csv`\n",
    "\n",
    "Use the data at these urls to **construct a fake news classifier.** \n",
    "\n",
    "1. Your model must be able to **make predictions** about whether or not an unseen news item is fake or real. \n",
    "2. Because fake news models must be able to make millions of predictions per day, it must be able to make predictions very quickly. More columns mean more computation time. **Your final model should use no more than 50 columns.** \n",
    "\n",
    "You are free to create any columns that you need, and to use any functions that we have or have not covered in the course. You may also use any machine learning model. \n",
    "\n",
    "Please use Markdown headers with \\#\\# signs to clearly distinguish the different stages of your solution. \n",
    "\n",
    "### Requirements\n",
    "\n",
    "1. Any operations that you perform multiple times (such processing that you perform on both the training and test sets) must be contained in function with informative docstrings. Comments and explanations are expected throughout. It is especially important to explain how you chose the columns to use in your final model. \n",
    "2. You should not use for-loops to iterate over the rows of data frames or arrays. \n",
    "3. You must fit your model on the training data, and not use the test data for fitting.\n",
    "\n",
    "### Hints\n",
    "\n",
    "- `pd.concat()` is a good way to combine data frames. \n",
    "- Try fitting a model with as many columns as you want first. See if you can get a representation of which columns are important, and then select your final columns from this list. \n",
    "- In class, we talked about greedy stagewise feature selection and exhaustive enumeration for determining a good set of columns. Neither of these methods are recommended for this problem. \n",
    "- If you want to be creative about your model choice, then please go for it. If you want a safe option, try logistic regression.\n",
    "- If a model takes too long to fit on the full data set, try fitting it on, say, 10% of the data. \n",
    "- You might find the some of the [cheatsheets](https://philchodrow.github.io/PIC16A/resources/) to be helpful. \n",
    "\n",
    "### Rubric\n",
    "\n",
    "- (**15 points**): clearly written code that makes economical use of skills from the course to manipulate data. \n",
    "- (**15 points**): comments, explanatory surrouding text, and docstrings for any functions and classes. \n",
    "- (**20 points**): computed according to the formula `20 x score`, where `score` is your model's prediction performance on unseen data. Models that use more than 50 columns can receive up to 15 of these points. Scores will be rounded up. For example, if you obtain an 84% predictive performance with 50 columns, then the score of `20 x 0.84 = 16.8` will be rounded up to 17 points. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
